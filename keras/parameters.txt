
1. number of hidden layers
2. size of each layer (probably should be constant for all layers)
3. PReLU
4. BatchNormalization
5. Dropout rate
6. optimizers:
   * SGD: learning rate, momentum neterov, decay
   * Adam
7. Batch size
8. number of epochs
9. activation functions: tanh, simoid, softsign
10. regularizations: L1, L2
11. weight initializer: unirom, glorot_uniform, etc
